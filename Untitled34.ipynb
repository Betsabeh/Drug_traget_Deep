{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "Gt9hMmM0-IcV"
      },
      "outputs": [],
      "source": [
        "#standard package\n",
        "import csv\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity, euclidean_distances\n",
        "\n",
        "\n",
        "import torch\n",
        "from transformers import AutoTokenizer, AutoModel\n",
        "from torch.utils.data import Dataset, DataLoader, random_split\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from tensorflow.keras import layers\n",
        "import math\n",
        "from tqdm.auto import tqdm\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "7Sss2USZrv5I"
      },
      "outputs": [],
      "source": [
        "def create_dict_from_csv(file_path):\n",
        "    result_dict = {}\n",
        "    All_id = []\n",
        "    All_features =[]\n",
        "    with open(file_path, mode='r', newline='', encoding='utf-8') as csvfile:\n",
        "        reader = csv.reader(csvfile)\n",
        "        for row in reader:\n",
        "            if len(row) >= 2:\n",
        "                cid = row[0].strip()\n",
        "                feature = row[1].strip()\n",
        "                result_dict[cid] = feature\n",
        "                All_id.append(cid)\n",
        "                All_features.append(feature)\n",
        "    return result_dict, All_id, All_features\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QHNy1zQr75j4",
        "outputId": "960dbb1e-4c77-4d9f-9abc-4401c2a469c5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "11485656 : CC1=CC(=C(C=C1)F)NC(=O)NC2=CC=C(C=C2)C3=C4C(=CC=C3)NN=C4N\n",
            "AAA61480.1|CLK1|CLK1 : MRHSKRTYCPDWDDKDWDYGKWRSSSSHKRRKRSHSSAQENKRCKYNHSKMCDSHYLESRSINEKDYHSRRYIDEYRNDYTQGCEPGHRQRDHESRYQNHSSKSSGRSGRSSYKSKHRIHHSTSHRRSHGKSHRRKRTRSVEDDEEGHLICQSGDVLSARYEIVDTLGEGAFGKVVECIDHKAGGRHVAVKIVKNVDRYCEAARSEIQVLEHLNTTDPNSTFRCVQMLEWFEHHGHICIVFELLGLSTYDFIKENGFLPFRLDHIRKMAYQICKSVNFLHSNKLTHTDLKPENILFVQSDYTEAYNPKIKRDERTLINPDIKVVDFGSATYDDEHHSTLVSTRHYRAPEVILALGWSQPCDVWSIGCILIEYYLGFTVFPTHDSKEHLAMMERILGPLPKHMIQKTRKRKYFHHDRLDWDEHSSAGRYVSRACKPLKEFMLSQDVEHERLFDLIQKMLEYDPAKRITLREALKHPFFDLLKKSI\n",
            "Number of Drugs= 72 Number of Proteins= 442\n"
          ]
        }
      ],
      "source": [
        "Drug_Data, Drug_cid,Drug_Smiles=create_dict_from_csv('drug_info.csv')\n",
        "Drug_cid = Drug_cid[1:]\n",
        "Drug_Smiles =Drug_Smiles[1:]\n",
        "NUM_Drugs= len(Drug_Smiles)\n",
        "print(Drug_cid[0],':',Drug_Smiles[0])\n",
        "\n",
        "\n",
        "Protein_Data, Protein_name, Protein_AA=create_dict_from_csv('prot_info.csv')\n",
        "Protein_name = Protein_name[1:]\n",
        "Protein_AA =Protein_AA[1:]\n",
        "NUM_Protein= len(Protein_AA)\n",
        "print(Protein_name[0],':',Protein_AA[0])\n",
        "\n",
        "print('Number of Drugs=', NUM_Drugs, 'Number of Proteins=', NUM_Protein)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 243,
          "referenced_widgets": [
            "8f2ddf69e9224856bb760a174eab0e99",
            "7f0ec56a74004ef386847d17d25b5e6a",
            "9329e6dc22274f559321fa0889eef71e",
            "0e5e6673da1644bcb41bcc2836b5067b",
            "2dfad207f8d34a1b8109bbd97673202c",
            "b177bd2aa4f64a8796b8a1e1cf5bb4b8",
            "1a450dc7037d40e9a449ae7a1126fc94",
            "43848e25b81c40a5920911b395716675",
            "074a6d9022ce4bc29c7849eb841ed4d8",
            "99fe4f78e37943b58ebe2a03b2fef75d",
            "2f4e80c96b714b03bfe101a1bccf4e24"
          ]
        },
        "id": "7BLUufu6fdGK",
        "outputId": "3a23c7c3-03d1-4f96-d9dc-f7fb699a8af2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "56\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "Processing Batches:   0%|          | 0/56 [00:00<?, ?it/s]"
            ],
            "application/vnd.jupyter.widget-view+json": {
              "version_major": 2,
              "version_minor": 0,
              "model_id": "8f2ddf69e9224856bb760a174eab0e99"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Total protein embeddings tensor shape: torch.Size([442, 512, 1024])\n",
            "Example of the first embedding's shape: torch.Size([8, 512, 1024])\n"
          ]
        }
      ],
      "source": [
        "def protein_embedding(protein_sequences):\n",
        "    \"\"\"\n",
        "    Embed a list of protein sequences (strings of amino acids) using ProtBert.\n",
        "\n",
        "    Args:\n",
        "        protein_sequences (list of str): List of protein amino acid sequences.\n",
        "\n",
        "    Returns:\n",
        "        embeddings (torch.Tensor): Embeddings of shape (batch_size, sequence_length, hidden_size).\n",
        "    \"\"\"\n",
        "\n",
        "    torch.cuda.empty_cache()\n",
        "    # Load the ProtBert tokenizer and mode\n",
        "    model_name = \"Rostlab/prot_bert\" #\"facebook/esm2_t6_8M_UR50D\"\n",
        "    tokenizer = AutoTokenizer.from_pretrained(model_name, do_lower_case=False)\n",
        "    model = AutoModel.from_pretrained(model_name)\n",
        "\n",
        "\n",
        "    # Move model to GPU if available\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "    #model.eval()\n",
        "\n",
        "    max_seq_len = 512\n",
        "    processed_sequences = []\n",
        "    for seq in protein_sequences:\n",
        "        seq_truncated = seq[:max_seq_len] # Apply truncation here\n",
        "        processed_sequences.append(\" \".join(list(seq_truncated)))\n",
        "\n",
        "    batch_size = 8\n",
        "    all_embeddings =[]\n",
        "    num_Batches = math.ceil(NUM_Protein/batch_size)\n",
        "    print(num_Batches)\n",
        "    # Iterate through sequences in batches\n",
        "    for i in tqdm(range(0, len(processed_sequences), batch_size), desc=\"Processing Batches\"):\n",
        "         #print(i)\n",
        "         batch_sequences = processed_sequences[i:i + batch_size]\n",
        "         # Tokenize the entire batch\n",
        "         # padding='longest': Pads to the length of the longest sequence in the current batch.\n",
        "         # truncation=True: Truncates sequences if they exceed the tokenizer's max input length (or max_seq_len if specified).\n",
        "         inputs = tokenizer(batch_sequences,return_tensors=\"pt\",padding='longest', truncation=True,\n",
        "                            max_length=max_seq_len).to(device)\n",
        "\n",
        "         with torch.no_grad():\n",
        "           outputs = model(**inputs)\n",
        "           # Get the last hidden states (batch_size, sequence_length, hidden_size)\n",
        "           batch_embeddings = outputs.last_hidden_state\n",
        "           #print(\"Protein embeddings size =\", batch_embeddings.shape)\n",
        "\n",
        "         # Move embeddings to CPU and append to the list\n",
        "         # Important: Only move to CPU if you don't need them on GPU for subsequent steps.\n",
        "         # Moving to CPU frees up GPU memory.\n",
        "         all_embeddings.append(batch_embeddings.cpu())\n",
        "\n",
        "    # Concatenate all batch embeddings into a single tensor\n",
        "    # This will result in a tensor of shape (total_sequences, max_length_in_batch, hidden_size)\n",
        "    # where max_length_in_batch is the maximum length of a sequence in *any* batch,\n",
        "    # because we padded 'longest' within each batch. If you need consistent length across ALL batches,\n",
        "    # you'd need to pad to `max_seq_len` for every batch.\n",
        "    final_embeddings_tensor = torch.cat(all_embeddings, dim=0)\n",
        "\n",
        "    print(f\"\\nTotal protein embeddings tensor shape: {final_embeddings_tensor.shape}\")\n",
        "    print(f\"Example of the first embedding's shape: {all_embeddings[0].shape}\")\n",
        "    return final_embeddings_tensor\n",
        "\n",
        "    #---------------------------------------------------------------------------\n",
        "    '''\n",
        "    # Prepare the sequences by adding spaces between amino acids as required by ProtBert tokenizer\n",
        "    sequences_with_spaces = [\" \".join(list(seq)) for seq in protein_sequences]\n",
        "\n",
        "    # Tokenize sequences (pads & truncates automatically)\n",
        "    inputs = tokenizer(sequences_with_spaces, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "        outputs = model(**inputs)\n",
        "\n",
        "    # Extract the last hidden state (embeddings)\n",
        "    embeddings = outputs.last_hidden_state '''\n",
        "\n",
        "\n",
        "Protein_full_embeddings = protein_embedding(Protein_AA)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "x2o_UGp9k4x6",
        "outputId": "5c3c3bee-afff-41ed-8cec-68ce5274ee2b"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Some weights of RobertaModel were not initialized from the model checkpoint at DeepChem/ChemBERTa-10M-MLM and are newly initialized: ['pooler.dense.bias', 'pooler.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drug embeding size= torch.Size([72, 89, 384])\n"
          ]
        }
      ],
      "source": [
        "def drug_embedding(Drug_Smiles):\n",
        "    tokenizer = AutoTokenizer.from_pretrained(\"DeepChem/ChemBERTa-10M-MLM\")\n",
        "    model = AutoModel.from_pretrained(\"DeepChem/ChemBERTa-10M-MLM\")\n",
        "    #move to gpu\n",
        "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "    model.to(device)\n",
        "\n",
        "    #tokenizer\n",
        "    inputs = tokenizer(Drug_Smiles, return_tensors=\"pt\", padding=True, truncation=True).to(model.device)\n",
        "\n",
        "    with torch.no_grad():\n",
        "      outputs = model(**inputs)\n",
        "      #print(outputs)\n",
        "\n",
        "    embeddings = outputs.last_hidden_state # Or pooler_output depending on your needs\n",
        "    print(\"Drug embeding size=\",embeddings.shape) # Should be (batch_size, sequence_length, hidden_size)\n",
        "    return embeddings\n",
        "\n",
        "Drug_full_embeddings = drug_embedding(Drug_Smiles)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Rsg37Hqm2Pz0",
        "outputId": "c3f74d45-a2e1-4607-c1d9-601f5a8f3c23"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Transformed to long format DataFrame with 31824 interactions.\n",
            "First 5 rows of transformed interaction data:\n",
            "    drug_id          protein_name  binding_db\n",
            "0  11314340  AAA61480.1|CLK1|CLK1         1.4\n",
            "1  10074640  AAA61480.1|CLK1|CLK1     10000.0\n",
            "2  11485656  AAA61480.1|CLK1|CLK1      8900.0\n",
            "3  24889392  AAA61480.1|CLK1|CLK1     10000.0\n",
            "4   6450551  AAA61480.1|CLK1|CLK1     10000.0\n"
          ]
        }
      ],
      "source": [
        "# --- 1. Read Data ---\n",
        "csv_file_name ='Davis.csv'\n",
        "raw_df = pd.read_csv(csv_file_name, header=None)\n",
        "#print(f\"\\nLoaded raw CSV with shape: {raw_df.shape}\")\n",
        "#print(\"Raw CSV head:\")\n",
        "#print(raw_df.head())\n",
        "\n",
        "# Extract drug CIDs from the first row (starting from the 5th column, index 4)\n",
        "# Assuming the first 4 columns of the first row are empty or irrelevant for drug CIDs\n",
        "drug_cids = raw_df.iloc[0, 4:].astype(str).tolist() # Convert to string to handle potential mixed types\n",
        "\n",
        "# Extract protein identifiers from the 4th column (index 3) of rows 2 onwards (index 2 in 0-based)\n",
        "# Assuming the combined ID is in the 4th column\n",
        "protein_identifiers = raw_df.iloc[3:, 3].astype(str).tolist()\n",
        "# Extract binding data (values start from row 2, column 4)\n",
        "binding_data_matrix = raw_df.iloc[3:, 4:]\n",
        "\n",
        "#print(\"CID=\",drug_cids,'\\n',\"protein names=\",protein_identifiers,'\\n',binding_data_matrix)\n",
        "\n",
        "# Convert wide format to long format\n",
        "long_data = []\n",
        "for p_idx, p_id in enumerate(protein_identifiers):\n",
        "    for d_idx, d_cid in enumerate(drug_cids):\n",
        "        binding_value = binding_data_matrix.iloc[p_idx, d_idx]\n",
        "        if pd.notna(binding_value): # Only include non-NaN binding values\n",
        "            long_data.append({\n",
        "                'drug_id': d_cid,\n",
        "                'protein_name': p_id, # Using the combined protein identifier as 'protein_name'\n",
        "                'binding_db': float(binding_value)\n",
        "            })\n",
        "        else :\n",
        "            long_data.append({\n",
        "                'drug_id': d_cid,\n",
        "                'protein_name': p_id, # Using the combined protein identifier as 'protein_name'\n",
        "                'binding_db': float(10000)\n",
        "            })\n",
        "\n",
        "#print (long_data)\n",
        "df_interactions = pd.DataFrame(long_data)\n",
        "print(f\"\\nTransformed to long format DataFrame with {len(df_interactions)} interactions.\")\n",
        "print(\"First 5 rows of transformed interaction data:\")\n",
        "print(df_interactions.head())\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 2. Create Mappings from IDs/Names to Embedding Indices ---\n",
        "# IMPORTANT ASSUMPTION: The order of drug CIDs extracted from the CSV\n",
        "# matches the order of drugs in drug_full_embeddings (index 0 to 71).\n",
        "# Similarly for protein identifiers and protein_full_embeddings.\n",
        "\n",
        "drug_id_to_idx = {cid: i for i, cid in enumerate(drug_cids)}\n",
        "protein_name_to_idx = {p_id: i for i, p_id in enumerate(protein_identifiers)}\n",
        "\n",
        "# Map drug_id and protein_name in DataFrame to their respective indices\n",
        "df_interactions['drug_idx'] = df_interactions['drug_id'].map(drug_id_to_idx)\n",
        "df_interactions['protein_idx'] = df_interactions['protein_name'].map(protein_name_to_idx)\n",
        "\n",
        "# Drop rows where mapping failed (if any drug_id or protein_name not found in embeddings)\n",
        "df_interactions.dropna(subset=['drug_idx', 'protein_idx'], inplace=True)\n",
        "df_interactions['drug_idx'] = df_interactions['drug_idx'].astype(int)\n",
        "df_interactions['protein_idx'] = df_interactions['protein_idx'].astype(int)\n",
        "\n",
        "print(f\"\\nAfter mapping, {len(df_interactions)} valid interactions remain.\")\n",
        "print(\"First 5 rows with mapped indices:\")\n",
        "print(df_interactions.head())"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "DuwnelzHYOOL",
        "outputId": "f469a5fa-d5ec-4db4-da3f-2e890a8a475b"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "After mapping, 31824 valid interactions remain.\n",
            "First 5 rows with mapped indices:\n",
            "    drug_id          protein_name  binding_db  drug_idx  protein_idx\n",
            "0  11314340  AAA61480.1|CLK1|CLK1         1.4         0            0\n",
            "1  10074640  AAA61480.1|CLK1|CLK1     10000.0         1            0\n",
            "2  11485656  AAA61480.1|CLK1|CLK1      8900.0         2            0\n",
            "3  24889392  AAA61480.1|CLK1|CLK1     10000.0         3            0\n",
            "4   6450551  AAA61480.1|CLK1|CLK1     10000.0         4            0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 3. No Embedding Pooling for 2D CNN Input ---\n",
        "# We will directly use the full 2D embedding matrices for drugs and proteins.\n",
        "# Drug_full_embeddings: (num_drugs, drug_seq_len, drug_embed_dim) -> (72, 89, 384)\n",
        "# Protein_full_embeddings: (num_proteins, protein_seq_len, protein_embed_dim) -> (442, 512, 1024)\n",
        "\n",
        "print(f\"\\nUsing full 2D drug embeddings shape: {Drug_full_embeddings.shape}\")\n",
        "print(f\"Using full 2D protein embeddings shape: {Protein_full_embeddings.shape}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uA29TWDwYszA",
        "outputId": "195e030c-55f9-49c1-c331-9b6cede013ef"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Using full 2D drug embeddings shape: torch.Size([72, 89, 384])\n",
            "Using full 2D protein embeddings shape: torch.Size([442, 512, 1024])\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "55sg8SQVnT1I",
        "outputId": "a287c5a4-d6cf-4d44-c710-6c2a0783e40c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "DTI Dataset have :31824 samples\n",
            "Example drug embedding matrix shape: torch.Size([1, 89, 384])\n",
            "Example protein embedding matrix shape: torch.Size([1, 512, 1024])\n",
            "Example binding affinity: 1.399999976158142\n"
          ]
        }
      ],
      "source": [
        "# --- 4. Define a PyTorch Dataset Class ---\n",
        "class DrugProteinDataset(Dataset):\n",
        "    def __init__(self, df, drug_full_embeddings, protein_full_embeddings):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            df (pd.DataFrame): DataFrame containing 'drug_idx', 'protein_idx', and 'binding_db'.\n",
        "            drug_full_embeddings (torch.Tensor): Full 2D drug embeddings (num_drugs, seq_len, embed_dim).\n",
        "            protein_full_embeddings (torch.Tensor): Full 2D protein embeddings (num_proteins, seq_len, embed_dim).\n",
        "        \"\"\"\n",
        "        self.df = df\n",
        "        self.drug_embeddings_full = drug_full_embeddings\n",
        "        self.protein_embeddings_full = protein_full_embeddings\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        # Get the row for the current index\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # Get drug and protein indices\n",
        "        drug_idx = row['drug_idx']\n",
        "        protein_idx = row['protein_idx']\n",
        "\n",
        "        # Retrieve the full 2D embedding matrices\n",
        "        drug_embed_matrix = self.drug_embeddings_full[drug_idx]\n",
        "        protein_embed_matrix = self.protein_embeddings_full[protein_idx]\n",
        "\n",
        "        # Get the binding affinity value\n",
        "        binding_affinity = torch.tensor(row['binding_db'], dtype=torch.float32)\n",
        "\n",
        "        # For 2D CNN, you typically need an input channel dimension.\n",
        "        # If your embeddings are already (Height, Width), a CNN expects (Channels, Height, Width).\n",
        "        # We add a channel dimension of 1 if treating each as a single-channel image.\n",
        "        drug_embed_matrix = drug_embed_matrix.unsqueeze(0) # Adds a channel dimension: (1, 89, 384)\n",
        "        protein_embed_matrix = protein_embed_matrix.unsqueeze(0) # Adds a channel dimension: (1, 512, 1024)\n",
        "\n",
        "        return drug_embed_matrix, protein_embed_matrix, binding_affinity\n",
        "\n",
        "\n",
        "DTI_Dataset = DrugProteinDataset(df_interactions, Drug_full_embeddings, Protein_full_embeddings)\n",
        "print(f\"DTI Dataset have :{len(DTI_Dataset)} samples\")\n",
        "print(f\"Example drug embedding matrix shape: {DTI_Dataset[0][0].shape}\") # (1, 89, 384)\n",
        "print(f\"Example protein embedding matrix shape: {DTI_Dataset[0][1].shape}\") # (1, 512, 1024)\n",
        "print(f\"Example binding affinity: {DTI_Dataset[0][2]}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 5. Split into Train and Test Sets ---\n",
        "train_size = int(0.8 * len(DTI_Dataset)) # 80% for training\n",
        "test_size = len(DTI_Dataset) - train_size # Remaining for testing\n",
        "\n",
        "train_dataset, test_dataset = random_split(DTI_Dataset, [train_size, test_size])\n",
        "\n",
        "print(f\"\\nTrain set size: {len(train_dataset)} samples\")\n",
        "print(f\"Test set size: {len(test_dataset)} samples\")\n",
        "\n",
        "\n",
        "# --- 6. Create PyTorch DataLoaders ---\n",
        "batch_size_loader = 8 # Adjust based on your GPU memory. Keep it smaller for larger inputs.\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size_loader, shuffle=True)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size_loader, shuffle=False)\n",
        "\n",
        "print(f\"\\nTrain DataLoader created with batch size {batch_size_loader}.\")\n",
        "print(f\"Test DataLoader created with batch size {batch_size_loader}.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QiZY3QlTaeZf",
        "outputId": "800defcf-ddb0-494f-cbde-0ed5755fe71a"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Train set size: 25459 samples\n",
            "Test set size: 6365 samples\n",
            "\n",
            "Train DataLoader created with batch size 8.\n",
            "Test DataLoader created with batch size 8.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 7. Define the 2D CNN Model in PyTorch ---\n",
        "class DrugProteinCNN(nn.Module):\n",
        "  def __init__(self,drug_input_shape, protein_input_shape):\n",
        "    super(DrugProteinCNN, self).__init__()\n",
        "\n",
        "    # Drug Branch CNN\n",
        "    # Input shape: (Batch_size, 1, 89, 384)\n",
        "    self.drug_cnn = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=1, out_channels=32, kernel_size=(3, 3), padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2)), # Output: (Batch, 32, 44, 192) approx\n",
        "        nn.Conv2d(in_channels=32, out_channels=64, kernel_size=(3, 3), padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2)) # Output: (Batch, 64, 22, 96) approx\n",
        "        )\n",
        "    # Calculate the flattened size after drug CNN\n",
        "    # We need to pass a dummy tensor to determine the output size\n",
        "    dummy_drug_input = torch.randn(1, *drug_input_shape) # (1, 1, 89, 384)\n",
        "    dummy_drug_output = self.drug_cnn(dummy_drug_input)\n",
        "    self.drug_flatten_size = dummy_drug_output.shape[1] * dummy_drug_output.shape[2] * dummy_drug_output.shape[3]\n",
        "    print(f\"Drug CNN flattened size: {self.drug_flatten_size}\")\n",
        "\n",
        "    # Protein Branch CNN\n",
        "    # Input shape: (Batch_size, 1, 512, 1024)\n",
        "    self.protein_cnn = nn.Sequential(\n",
        "        nn.Conv2d(in_channels=1, out_channels=16, kernel_size=(5, 5), padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2)), # Output: (Batch, 32, 256, 512) approx\n",
        "        nn.Conv2d(in_channels=16, out_channels=32, kernel_size=(3, 3), padding='same'),\n",
        "        nn.ReLU(),\n",
        "        nn.MaxPool2d(kernel_size=(2, 2)), # Output: (Batch, 64, 128, 256) approx\n",
        "        #nn.Conv2d(in_channels=128, out_channels=256, kernel_size=(3, 3), padding='same'),\n",
        "        #nn.ReLU(),\n",
        "        #nn.MaxPool2d(kernel_size=(2, 2)) # Output: (Batch, 256, 64, 128) approx\n",
        "        )\n",
        "    # Calculate the flattened size after protein CNN\n",
        "    dummy_protein_input = torch.randn(1, *protein_input_shape) # (1, 1, 512, 1024)\n",
        "    dummy_protein_output = self.protein_cnn(dummy_protein_input)\n",
        "    self.protein_flatten_size = dummy_protein_output.shape[1] * dummy_protein_output.shape[2] * dummy_protein_output.shape[3]\n",
        "    print(f\"Protein CNN flattened size: {self.protein_flatten_size}\")\n",
        "\n",
        "    # Fully Connected Layers for combined features\n",
        "    self.fc_combined = nn.Sequential(\n",
        "        nn.Linear(self.drug_flatten_size + self.protein_flatten_size, 256),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(256, 128),\n",
        "        nn.ReLU(),\n",
        "        nn.Dropout(0.5),\n",
        "        nn.Linear(128, 1) # Output a single binding affinity value\n",
        "     )\n",
        "\n",
        "  def forward(self, drug_matrices, protein_matrices):\n",
        "      # Process drug matrices\n",
        "      drug_features = self.drug_cnn(drug_matrices)\n",
        "      drug_features = torch.flatten(drug_features, 1) # Flatten for FC layers\n",
        "\n",
        "      # Process protein matrices\n",
        "      protein_features = self.protein_cnn(protein_matrices)\n",
        "      protein_features = torch.flatten(protein_features, 1) # Flatten for FC layers\n",
        "\n",
        "      # Concatenate features from both branches\n",
        "      combined_features = torch.cat((drug_features, protein_features), dim=1)\n",
        "\n",
        "      # Pass through fully connected layers\n",
        "      output = self.fc_combined(combined_features)\n",
        "      return output\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_Kf-jkjaeIfB"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "sample_drug_matrix, sample_protein_matrix, _ = DTI_Dataset[0]\n",
        "drug_input_shape = sample_drug_matrix.shape # (1, 89, 384)\n",
        "protein_input_shape = sample_protein_matrix.shape # (1, 512, 1024)\n",
        "\n",
        "model = DrugProteinCNN(drug_input_shape, protein_input_shape)\n",
        "\n",
        "# Move model to GPU if available\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model.to(device)\n",
        "print(f\"\\nModel instantiated and moved to {device}.\")\n",
        "print(model)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aZAGskgVhavF",
        "outputId": "fd4d946f-1dcc-47c7-8a32-d96e3a5d62cc"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drug CNN flattened size: 135168\n",
            "Protein CNN flattened size: 1048576\n",
            "\n",
            "Model instantiated and moved to cuda.\n",
            "DrugProteinCNN(\n",
            "  (drug_cnn): Sequential(\n",
            "    (0): Conv2d(1, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(32, 64, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (protein_cnn): Sequential(\n",
            "    (0): Conv2d(1, 16, kernel_size=(5, 5), stride=(1, 1), padding=same)\n",
            "    (1): ReLU()\n",
            "    (2): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "    (3): Conv2d(16, 32, kernel_size=(3, 3), stride=(1, 1), padding=same)\n",
            "    (4): ReLU()\n",
            "    (5): MaxPool2d(kernel_size=(2, 2), stride=(2, 2), padding=0, dilation=1, ceil_mode=False)\n",
            "  )\n",
            "  (fc_combined): Sequential(\n",
            "    (0): Linear(in_features=1183744, out_features=256, bias=True)\n",
            "    (1): ReLU()\n",
            "    (2): Dropout(p=0.5, inplace=False)\n",
            "    (3): Linear(in_features=256, out_features=128, bias=True)\n",
            "    (4): ReLU()\n",
            "    (5): Dropout(p=0.5, inplace=False)\n",
            "    (6): Linear(in_features=128, out_features=1, bias=True)\n",
            "  )\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# --- 8. Training Loop ---\n",
        "# Loss function for regression (Mean Squared Error)\n",
        "criterion = nn.MSELoss()\n",
        "# Optimizer (Adam is a good general choice)\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "num_epochs = 2\n",
        "\n",
        "print(\"\\n--- Starting Training ---\")\n",
        "for epoch in range(num_epochs):\n",
        "    model.train() # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "    for drug_matrices, protein_matrices, targets in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} Training\"):\n",
        "        # Move data to the same device as the model\n",
        "        drug_matrices = drug_matrices.to(device)\n",
        "        protein_matrices = protein_matrices.to(device)\n",
        "        targets = targets.to(device)\n",
        "\n",
        "        # Zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # Forward pass\n",
        "        outputs = model(drug_matrices, protein_matrices)\n",
        "\n",
        "        # Calculate loss\n",
        "        loss = criterion(outputs.squeeze(), targets) # .squeeze() to remove singleton dimension from output\n",
        "\n",
        "        # Backward pass and optimize\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item() * drug_matrices.size(0) # Accumulate loss\n",
        "\n",
        "    epoch_loss = running_loss / len(train_dataset)\n",
        "    print(f\"Epoch [{epoch+1}/{num_epochs}], Training Loss: {epoch_loss:.4f}\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 211
        },
        "id": "FU_j2MY6oQAO",
        "outputId": "3b2cc6b2-152a-4f90-cee2-0a0ee6182f7a"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "NameError",
          "evalue": "name 'nn' is not defined",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
            "\u001b[0;32m/tmp/ipython-input-1-1263054746.py\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;31m# --- 8. Training Loop ---\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;31m# Loss function for regression (Mean Squared Error)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mcriterion\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mMSELoss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0;31m# Optimizer (Adam is a good general choice)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0moptimizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0moptim\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mAdam\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mparameters\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlr\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0.001\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mNameError\u001b[0m: name 'nn' is not defined"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": [],
      "authorship_tag": "ABX9TyOyVF2pO7VTzvyyIS/8cUB+"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "8f2ddf69e9224856bb760a174eab0e99": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_7f0ec56a74004ef386847d17d25b5e6a",
              "IPY_MODEL_9329e6dc22274f559321fa0889eef71e",
              "IPY_MODEL_0e5e6673da1644bcb41bcc2836b5067b"
            ],
            "layout": "IPY_MODEL_2dfad207f8d34a1b8109bbd97673202c"
          }
        },
        "7f0ec56a74004ef386847d17d25b5e6a": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_b177bd2aa4f64a8796b8a1e1cf5bb4b8",
            "placeholder": "​",
            "style": "IPY_MODEL_1a450dc7037d40e9a449ae7a1126fc94",
            "value": "Processing Batches: 100%"
          }
        },
        "9329e6dc22274f559321fa0889eef71e": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_43848e25b81c40a5920911b395716675",
            "max": 56,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_074a6d9022ce4bc29c7849eb841ed4d8",
            "value": 56
          }
        },
        "0e5e6673da1644bcb41bcc2836b5067b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_99fe4f78e37943b58ebe2a03b2fef75d",
            "placeholder": "​",
            "style": "IPY_MODEL_2f4e80c96b714b03bfe101a1bccf4e24",
            "value": " 56/56 [00:48&lt;00:00,  1.45it/s]"
          }
        },
        "2dfad207f8d34a1b8109bbd97673202c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "b177bd2aa4f64a8796b8a1e1cf5bb4b8": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "1a450dc7037d40e9a449ae7a1126fc94": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "43848e25b81c40a5920911b395716675": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "074a6d9022ce4bc29c7849eb841ed4d8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "99fe4f78e37943b58ebe2a03b2fef75d": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "2f4e80c96b714b03bfe101a1bccf4e24": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}